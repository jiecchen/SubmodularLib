\section{Submodularity}
\label{sec:submodularity}
In this section, we first give several equivalent definitions of submodularity, and then we introduce several fundamental properties of submodular functions. We also discuss various constraints that occur frequently in submodular optimization problems. In the last part of this section, we cover algorithms that solve constrained submodular maximization problems with theoretical approximation guarantee. 

\subsection{Definitions}
There are many equivalent definitions, and we discuss three of them in this section. 

\begin{definition}[submodular concave]
  \label{def:sub-concave}
  A function $f:~2^V \rightarrow \bbR$ is \emRed{submodular} if for any $A, B \subseteq V$, we have that:
  \begin{equation}
    \label{eq:sub-concave}
    f(A) + f(B) \geq f(A \cup B) + f(A \cap B).
  \end{equation}
\end{definition}

An alternate equivalent definition is more interpretable in many situations,

\begin{definition}[diminishing returns]
  \label{def:sub-diminishing}
  A function $f: 2^V \rightarrow \bbR$ is \emRed{submodular} if for any $A \subseteq B \subset V$, and $v \in V\backslash B$, we have that:
  \begin{equation}
    \label{eq:sub-diminishing}
    f(A + v) - f(A) \geq f(B + v) - f(B).
  \end{equation}
\end{definition}

Intuitively, this definition requires that the incremental ``gain'' of adding a new element $v$ decreases (diminishes) as the base set grows from $A$ to $B$. We will see that this property is actually shared by many real-world phenomenons.

It turns out that a stronger but equivalent statement can also serve as the definition of a submodular function,

\begin{definition}[group diminishing returns]
  \label{def:sub-diminishing-group}
  A function $f: 2^V \rightarrow \bbR$ is \emRed{submodular} if for any $A \subseteq B \subset V$, and $C \subseteq V\backslash B$, we have that:
  \begin{equation}
    \label{eq:sub-diminishing-group}
    f(A \cup C) - f(A) \geq f(B \cup C) - f(B).
  \end{equation}
\end{definition}


\subsection{Modularity and Supermodularity}
We also briefly mention modularity and supermodularity here. These two concepts are closely related to submodularity. 

A function $f: 2^V \rightarrow \bbR$ is modular if we replace inequality by equality in Definition \ref{def:sub-diminishing} (or any of other two). Formally, 

\begin{definition}[Modularity]
  \label{def:modular}
  A function $f: 2^V \rightarrow \bbR$ is \emRed{modular} if for any $A \subseteq B \subset V$, and $v \in V\backslash B$, we have that:
  \begin{equation}
    \label{eq:modular}
    f(A + v) - f(A) = f(B + v) - f(B).
  \end{equation}
\end{definition}
Notably, a modular function $f$ can always be written as
$$f(S) = f(\emptyset) + \sum_{v\in S} \left( f(\{v\}) - f(\emptyset) \right)$$
for any $S \subseteq V$. If we further assume $f(\emptyset) = 0$ (in this case, we call $f$ \emRed{normalized} or \emRed{proper}), we have a simplified expression,

$$f(S) = \sum_{v\in S} f(\{v\}).$$
We can interpret a normalized modular function as a weight function: each element in the ground set is assigned a weight and given a set $S \subseteq V$, the modular function returns sum of weights in that set.



Modularity can be useful in our discussion of submodularity, because modular functions can be used to construct submodular functions with desired properties in many applications. Examples can be found in e.g. \cite{LB11,LB11word}.




A supermodular function is defined by flipping the inequality sign in the definition of a submodular function. Formally,
\begin{definition}[Supermodularity]
  \label{def:supermodular}
  A function $f: 2^V \rightarrow \bbR$ is \emRed{supermodular} if for any $A \subseteq B \subset V$, and $v \in V\backslash B$, we have that:
  \begin{equation}
    \label{eq:submodular}
    f(A + v) - f(A) \leq f(B + v) - f(B).
  \end{equation}
\end{definition}

We will focus on submodular functions because a function is supermodular if and only if its negative is submodular. 



\subsection{Properties}
Like convex and concave functions, submodular functions have many nice properties. Lov{\'a}sz's comment on convex functions \cite{L83} gives an accurate description of submodular functions as well. We would like to quote it here, 
\begin{quote}
 - Convex functions occur in many mathematical models in economy,
engineering, and other sciences. Convexity is a very natural property
of various functions and domains occurring in such models; quite
often the only non-trivial property which can be stated in general.

- Convexity is preserved under many natural operations and
transformations, and thereby the effective range of results can be
extended, elegant proof techniques can be developed as well as
unforeseen applications of certain results can be given.

- Convex functions and domains exhibit sufficient structure so that a
mathematically beautiful and practically useful theory can be
developed.

- There are theoretically and practically (reasonably) efficient methods
to find the minimum of a convex function.
\end{quote}

We survey several useful properties which can be useful in our later sections. More properties of submodularity can be found in e.g. \cite{B14,F05}.





Submodularity is closed under addition, formally,
\begin{property}
  \label{prop:addition}
  Let $f_1, f_2: 2^V \rightarrow \bbR$ be two submodular functions. Then 
  $$f: 2^V\rightarrow \bbR~{with}~ f(A) = \alpha f_1(A) + \beta f_2(A)$$ 
is submodular for any fixed $\alpha, \beta \in \bbR^+$.
\end{property}


Adding a modular function does not break submodularity,
\begin{property}
  \label{prop:modular}
  Let $f_1, f_2: 2^V \rightarrow \bbR$, $f_1$ is submodular and $f_2$ is modular. Then
  $$f: 2^V \rightarrow \bbR~\text{with}~ f(A) = f_1(A) + \alpha f_2(A)$$
is submodular for any fixed $\alpha \in \bbR$.
\end{property}

Submodularity is preserved under restriction,
\begin{property}
  \label{prop:restriction}
  Let $f: 2^V \rightarrow \bbR$ be a submodular function. Let $S\subseteq V$ be a fixed set. Then
$$f':2^V \rightarrow \bbR~{with}~f'(A) = f(A\cap S)$$
is submodular.
\end{property}

As a direct implication of Property \ref{prop:addition} and Property \ref{prop:restriction}, we have the following more general result,
\begin{property}
Let $f:2^V \rightarrow \bbR$ be a submodular function, $\calC = \{C_1, C_2, \ldots, C_k\}$ be a collection of subsets of $V$ (i.e. each $C_i \subseteq V$). Then
$$f':2^V \rightarrow \bbR~{with}~f'(A) = \sum_{C\in\calC}f(A\cap C)$$ 
is submodular.
\end{property}
This property can be useful in graphical models and image processing. \chensays{TODO: show examples}


The following property can be useful when we show that the objective function of k-medoid problem is supermodular,
\begin{property}
  \label{prop:max}
Consider $V$ as a set of indices. Let $\mathbf{c}\in \bbR^V$ be a fixed vector, $c_i$ its $i$th coordinate. Then 
$$f:2^V \rightarrow \bbR~{with}~ f(A) = \max_{j\in A}c_i$$ 
is submodular.
\end{property}

We can use non-negative modular function and a concave function to construct submodular functions,
\begin{property}
  Let $m: 2^V \rightarrow \bbR^+$ be a modular function, and $f$ a concave function over $\bbR$. Then
$$f: 2^V \rightarrow \bbR ~{with}~ f(A) = g(m(A))$$
is submodular.
\end{property}

Before introducing the next property, we define the monotonitcity of set function,
\begin{definition}[Monotonitcity]
  An set function $f: 2^V \rightarrow  \bbR$ is said to be non-decreasing if for any $A\subseteq B \subseteq V$, $f(A) \leq f(B)$. Non-increasing set functions are defined in the similar way.
\end{definition}
When we say a submodular function is monotone, we mean it is non-decreasing.

\begin{property}
  Let $f, g: 2^V \rightarrow \bbR$ be two submodular functions. If $(f - g)(\cdot)$ is either non-decreasing or non-increasing, then $f: 2^V \rightarrow \bbR$ with
$$f(A) = \min(f(A), g(A))$$
is submodular.
\end{property}



\subsection{Constraints}
Now we discuss the constraints in submodular optimization problems. A submodular maximization problem usually has the following form,
\begin{equation}
  \label{eq:optimization}
  \argmax_{I\in\calI} f(I)
\end{equation}

where $f$ is a submodular function and $\calI \subseteq 2^V$ is the collection of all feasible solutions. We call $\calI$ the \emRed{constraint} of the optimization problem. The structure of $\calI$ plays a crucial role in submodular optimization. Different constraints have different hardness results, normally the difficulty increases when the constraint becomes more general. In this section, we define several of the most popular constraints. Before introducing any of them, we define
\begin{definition}[Hereditary]
  \label{def:hereditary}
  A constraint $\calI \subseteq 2^V$ is said to be \emRed{hereditary} if 
$$I\in\calI \implies J\in\calI ~\text{for any}~J\subseteq I.$$
A hereditary constraint is sometimes called an \emRed{independent system} and each $I\in\calI$ is called an \emRed{independent set}. 
\end{definition}
All constraints we discuss in this section are hereditary.


\subsubsection{Cardinality Constraint}
Cardinality constraint is perhaps the most straightforward yet most popular constraint we would discuss in this survey. Efficient algorithms have been developed for finding or approximating the optimal solution of (\ref{eq:optimization}) subject to this constraint. There are also a lot of extensions, to both streaming and distributed setting. 

A \emRed{cardinality constraint} is parameterized with a fixed constant $k \in \bbZ^+$. It is simply defined as $\calI = \{A \subseteq V ~|~ |V| \leq k\}$, i.e. all subsets of $V$ with size no larger than $k$. Cardinality constraint is arguably the most popular constraint, and it occurs everywhere. For example, in k-medoid clustering, we want to find a set $S$ of \emph{at most} $k$ points, that minimizes the total distance of all points to $S$.  

\subsubsection{Knapsack Constraint}
Knapsack constraint generalizes cardinality constraint by assigning each element in $V$ a weight. Given a budget $W > 0$ and assume that each $i \in V$ is assigned a weight $w_i \geq 0$, a \emRed{knapsack constraint} can be defined as $\calI = \{S \subseteq V ~|~ \sum_{i\in S} w_i \leq W \}$.


\subsubsection{Matroid}
Informally,  \emRed{matroid} is the abstraction of the \emph{independence} concept in linear algebra. In fact there are so many results around matroid and the matroid itself becomes a subfield of algebra. We cover some basics of matroid theory, from which readers can easily see how powerful this concept is. 

Before discussing the concept of  matroid, we briefly review the independence concept from linear algebra. For simplicity, let us just consider $\bbR^d$ instead of a general linear space. A subset $S$ of $\bbR^d$ is said to be \emph{independent} if there does not exist any $\mathbf{x}\in S$ such that $\mathbf{x}$ can be represented by linear combination of vectors in $S\backslash\{\mathbf{x}\}$. 

Let $\calI = \{S \subseteq \bbR^d~|~S ~\text{is independent}~\}$, i.e. only a independent set can be considered feasible. From what we learn in college linear algebra course, we know $\calI$ has the following propertie: 1) if $I\in \calI$, any of $I$'s subsets is also in $\calI$ (or equivalently, $\calI$ is hereditary); 2) if $J, I\in \calI$ and $J$ has smaller size than $I$, we must be able to find an element  $\mathbf{x} \in I\backslash J$ such that $J\cup \{\mathbf{x}\} \in \calI$. 

Even for the ``trivial'' size function $f: 2^V \rightarrow \bbZ$ with $f(A) = |A|$, solving $\argmax_{I\in\calI} f(S)$ would have tremendous applications because its optimal solution is a base of the vector space.  If we somehow generalize the definition of independence, we may be able to model a much more larger class of problems into the form (\ref{eq:optimization}). 

It turns out that the properties of $\calI$ we just described are sufficient to give a meaningful definition for Matroid. Formally, 
\begin{definition}[Matroid]
\label{def:matroid}
  A set system $(V, \calI)$ is a \emRed{matroid} if it has the following properties,
  \begin{enumerate}
  \item $\forall I \in \calI$, $J\subseteq I \implies J\in \calI$ (i.e. hereditary)
  \item $\forall I, J \in \calI,$ with $|I| = |J| + 1$,  then $\exists~ x\in I\backslash J$ such that $J \cup \{x\}\in \calI$ 
  \end{enumerate} 
\end{definition}
Unlike in the $\bbR^d$ case, in this survey we restrict our discuss on matriod with a finite ground set. Note that in general, the intersection of several matroids is no longer a matroid.


Finally, we generalize the concept of \emRed{rank} in linear algebra. Let $(V, \calI)$ be a matroid, we define the rank function $r: 2^V \rightarrow \bbZ$ as $f(S) = \max_{I\subseteq S, I\in \calI} |I|$, i.e. the rank of $S\subseteq V$ is the maximum possible size of $S$'s subsets that are also members of $\calI$ (or in other words, independent). Our definition of rank is consistent with what we have in linear algebra (in that case $\calI$ is the collection of all independent sets). A rank function is submodular (as you may expect).


\subsubsection{Matching}
A \emRed{matching} of a graph $G = (V, E)$ is a set  $S\subseteq E$ such that no edges in $S$ share common vertex. 



\subsubsection{p-Matchoid}
Let $\calM_1 = (V_1, \calI_1),\ldots, \calM_q = (V_q, \calI_q)$ be $q$ matroids where $V = V_1\cup\ldots V_q$. Let $\calI = \{S\subseteq V~|~ S\cap V_i \in \calI_i ~\text{for all}~ i\}$. The finite set system $(V, \calI)$ is a \emRed{p-matchoid} if for every $e\in V$, $e$ is a member of $V_i$ for at most $p$ indices $i \in [q]$. 

The concept of p-matchoid generalizes the intersection of matroids (taking $p = q$ and $V_i = V$ for all $i$), and matching (which is $2$-matchoid). 




\subsubsection{$p$-System}
\emRed{$p$-system} is the most general constraint we will discuss in this survey, it includes graph matching, $p$-matchoid (therefore matroid) and many others as special cases.

Let $(V, \calI)$ be a set system and $\calI$ hereditary. An independent set $I$ (i.e. $I\in\calI$) is called a base of $A \subseteq V$ if $I\subseteq A$ and for any $e \in A\backslash I$, $I + e\not\in \calI$.  Let $\mathcal{B}(A)$ be the collection of all bases of $A$, we call $(V, \calI)$ a $p$-system if it further satisfies the following property,

$$\forall A\subseteq V: \max_{S\in\mathcal{B}(A)}|S| \leq p\cdot \min_{S\in\mathcal{B}(A)}|S|.$$

It is known that $p$-matchoid is $p$-system (therefore matching is $2$-system) and the intersection of $p$ matroids $(V, \calI_1), \ldots, (V, \calI_p)$ is $p$-system. 





\subsection{Algorithms for Submodular Maximization}
There are a lot of results for submodular maximization in the centralized setting where the data can fit into the main memory. Those results normally assume the \emRed{oracle model}: one is given a value oracle and a membership oracle. Given $S\subseteq V$, the membership oracle answers if $S \in \calI$ and the value oracle returns $f(S)$. We cover several classical results which serve as the building blocks for distributed/streaming algorithms for submodular maximization. A summary of best known algorithms (in terms of approximation ratio) for various of constraints is presented in Table \ref{table:centralized}.
\begin{table}[t]
\centering
\begin{tabular}{|l|l|l|}
\hline
constraint & monotone  &  non-negative \\
\hline
cardinality & $1 - 1/e$  \cite{NWF78} & $1/e + .004$ \cite{BFN+14}\\
\hline
matroid & $1 - 1/e$ \cite{CCP+11}, R & $\frac{1 - \eps}{e}$ \cite{FNS11}, R \\
\hline
matching & $\frac{1}{2 + \eps}$ \cite{FNS+11} & $\frac{1}{4 + \eps}$ \cite{FNS+11}\\
\hline
intersection of $p$ matroids & $\frac{1}{p + \eps}$ \cite{LSV10} & $\frac{p - 1}{p^2 + \eps}$ \cite{LSV10}\\
\hline
$p$-matchoid & $\frac{1}{p + 1}$ \cite{CCP+11,NWF78} & $\frac{(1-\eps)(2-o(1))}{e\cdot p}$ \cite{FNS+11,VCZ11}, R\\
\hline
\end{tabular}
\caption{Best known approximation bounds for submodular maximization in RAM model. Bounds for randomized algorithms that hold in expectation are marked (R).}
\label{table:centralized}
\end{table}






We introduce the notation for \emRed{marginal gain}: $\Delta_f(e|S) = f(S + e) - f(S)$, and we say an algorithm for sumbodular maximization gives \emRed{$\alpha$-approximation} if the solution $S$ it returns always has guarantee $f(S) \geq \alpha \cdot \argmax_{I\in\calI}f(I)$. When the algorithm is randomized, we normally say it guarantees \emRed{$\alpha$-approximation in expectation} if $\E[f(S)] \geq \alpha\cdot \argmax_{I\in\calI}f(I)$.


The following algorithm shows a popular greedy strategy for submodular maximization.

\begin{algorithm}[H]
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
\KwIn{$V$ the ground set, $f$ the submodular function, $\calI$ the constraint}
\KwOut{a set $S \subseteq V$}
$S \gets \emptyset$\;
\While{$\exists ~e\in V\backslash S$ s.t. $S\cup\{e\}\in \calI$} {
  $e \gets \argmax_{e\in V\backslash S, ~S\cup\{e\}\in \calI} \Delta_f(e|S)$\;\label{line:emax}
  $S \gets S\cup \{e\}$\;
}
\Return{$S$}\;
\caption{{\sc Greedy} algorithm for submodular maximization}
\label{algo:greedy}
\end{algorithm}


\subsubsection{Algorithms for Cardinality Constraint}


A celebrated result of \cite{NWF78} shows that,
\begin{theorem}[\cite{NWF78}]
  \label{thm:1978}
  For a non-negative monotone submodular function $f: 2^V \rightarrow \bbR$, let $\calI$ be the cardinality constraint, Algorithm \ref{algo:greedy} returns a $(1 - 1/e)$-approximation to $\argmax_{I\in \calI} f(S)$.
\end{theorem}
For several classes of submodular functions, this result is actually the best one can expect for any efficient algorithm (which we mean using only sub-exponential value queries). In fact the hardness result in \cite{NWF78,F98} shows that any algorithm that is only allowed sub-exponential number of value queries can not achieve better than $(1 - 1/e)$-approximation (for a large class of submodular functions).

There are several papers improving the running time of Algorithm \ref{algo:greedy} under cardinality constraint of size $k$ (under which the membership oracle is trivial). Minoux \cite{M78} proposed {\sc Lazy-greedy} as a fast implementation for Algorithm \ref{algo:greedy}. Instead of computing $\Delta_f(e|S)$ for each $e\in V\backslash S$ in Line \ref{line:emax},  {\sc Lazy-greedy} keeps an upper bound $\rho(e)$ (initially $+\infty$) on the marginal gain sorted in decreasing order (or kept in a heap). In each iteration, the {\sc Lazy-greedy} algorithm evaluates the element on top of the heap and updates its upper bound via $\rho(e) \gets \Delta(e|S)$. If the updated $\rho(e) \geq \rho(e')$ for all other $e'$, submodularity guarantees that $e$ is the element with the largest marginal gain. The exact number of value queries consumed by {\sc Lazy-greedy} is unknown because it heavily relies on both $f$ and $V$, experimental study however shows that the {\sc Lazy-greedy} algorithm is in order of magnitude faster than the naive implementation of Algorithm \ref{algo:greedy}.  

Wei et al. \cite{WIB14} improved the running time of {\sc Lazy-greedy} by approximating the underlying submodular function with a set of (sub)modular functions. Badanidiyuru et al. \cite{BV14} proposed a different approach that uses only $O(\frac{|V|}{\eps}\log \frac{1}{\eps})$ number of value queries and guarantees $(1 - 1/e - \eps)$-approximation. This result was improved by Mirzasoleiman et al. \cite{MBK+15} recently where they proposed a randomized algorithm ({\sc Stochastic-Greedy}) that reduces the number of queries to  $O(|V| \log \frac{1}{\eps})$ and the approximation guarantee is $(1 - 1/e - \eps)$, in expectation. The key observation made in \cite{MBK+15} is that, instead of considering all $e\in V\backslash S$, one can only consider $O(\frac{|V|}{k}\log \frac{1}{\eps})$  random samples from $V\backslash S$. 



 







\subsubsection{Algorithms for Matroid and $p$-System}
Now we consider the matroid constraint and $p$-system. In his classical paper \cite{J76}, Jenkyns proved that for a non-negative \emph{modular} maximization problem s.t. $p$-system, Algorithm \ref{algo:greedy} produces a $p$-approximation of the optimum. Since matroid is $1$-system, Algorithm \ref{algo:greedy} always gives the optimal solution. Remarkably, we the following much stronger result, 

\begin{theorem}[see e.g. \cite{B14,F05}]
  \label{thm:matroid}
  Let $(V, \calI)$ be a set system we consider, $\calI$ is a matroid \emph{if and only if} for \emph{any} non-negative modular function $f$, Algorithm \ref{algo:greedy} leads to a optimal solution for $\argmax_{I\in \calI} f(I)$.
\end{theorem}
Note that Algorithm \ref{algo:greedy} actually includes many greedy algorithms as special cases (e.g. maximum weighted spanning tree algorithm). The statement of Theorem \ref{thm:matroid} is so strong that it provides a complete  characterization for a large class of problems.

When $f$ is a non-negative non-decreasing submodular function, greedy strategy works well for $p$-system,
\begin{theorem}[\cite{NWF78,CCP+11}]
  \label{thm:}
  For a non-negative non-decreasing submodular function $f$, given a $p$-system $(V, \calI)$, Algorithm \ref{algo:greedy} returns a $\frac{1}{p + 1}$-approximation.
\end{theorem}
This theorem implies immediately that Algorithm \ref{algo:greedy} gives $1/2$-approximation for matroid constraint. 

There are several recent work improving the approximation ratio for matroid constraint (for non-negative monotone submodular functions): based on the idea of continuous greedy process \cite{V08} and pipage rounding \cite{AS04}, Calinescu et al. \cite{CCP+11} improved the approximation ratio to $(1 - 1/e)$ (in expectation); Filmus et al. \cite{FW14} presented a randomized combinatorial $(1 - 1/e -\eps)$-approximation algorithm using only $O(|V|r^3\eps^{-3}\log r)$ number of value queries, where $r$ is the size of returned set. Their method is based on local-search and is conceptually much simpler than \cite{CCP+11}; Badanidiyuru et al. \cite{BV14} gave an algorithm that runs in $O(\frac{r|V|}{\eps^4}\log^2\frac{r}{\eps})$ queries by using a variant of the continuous greedy algorithm.







Non-monotone submodular is normally more difficult to efficiently optimize. We will not discuss in this survey but simply present a summary in Table \ref{table:centralized}.













