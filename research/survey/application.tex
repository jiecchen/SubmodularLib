\section{Applications}
\label{sec:applications}
In this section, we first show how one can re-formulate many classical combinatorial problems as submodular maximization problems. We then show applications  from several resent papers from areas of social network, natural language processing, and general machine learning. 




% \subsection{List of Possible Applications}
% \begin{itemize}
% \item Combinatorial Problems: set cover, max $k$ coverage, vertex cover, edge cover, graph cut problems etc.
% \item Networks: social networks, viral marketing, diffusion networks etc.
% \item Graphical Models: image segmentation, tree distributions, factors etc.
% \item NLP: document summarization, web search, information retrieval
% \item Machine Learning: active/semi-supervised learning etc.
% \item Economics: markets, economies of scale
% \end{itemize}



\subsection{Classical Problems Revisited}
\label{sec:classical}
We first show that several well-known problems actually fit into our standard submodular maximization framework.

\paragraph{Exemplar Based Clustering}
Clustering is one of the most important tasks in the area of data mining. In the k-median problem \cite{KR09} one tries to minimize the sum of pairwise dissimilarities/distances between exemplars and the elements of the dataset. Let $d: V \times V \rightarrow \bbR^+\cup\{0\}$ be a function that measures the pairwise dissimilarity, we define the k-median loss function as
$$L(S) = \sum_{e\in V} \min_{v\in S} d(e, v).$$

It is quite straightforward (by Property \ref{prop:addition}, \ref{prop:max}) to show that $-L(S)$ is submodular. By introducing an auxiliary element $e_0$, we can transform $L$ into a non-negative monotone submodular function,
$$f(S) = L(\{e_0\}) - L(S \cup \{e_0\}).$$ 

A k-median problem can then be formulated as a submodular maximization problem subject to a cardinality constraint,

$$\argmax_{S\subseteq V: |S| \leq k} f(S).$$


\paragraph{Set Cover Problem}
The \emph{set cover problem} is an important problem in combinatorial optimization where we are given a collection of subsets of a set $E$, i.e. $V = \{C_1, C_2, \ldots, C_n\}$ where each $C_i \subseteq E$. We define a function $f:2^V \rightarrow \bbR$ such that $f(S) = |\cup_{C\in S} C|$. We can interpret $f$ as follow: given $S$ as a subset of $V$, the value of $f(S)$ is the number of distinct elements covered by the sets in $S$.

One can easily verify that $f$ satisfies the diminishing return property thus is a submodular function. Furthermore, it is clear that $f$ is non-decreasing.  Now given the cardinality constraint, we want to solve the following,

$$\argmax_{S\subseteq V: |S|\leq k} f(S).$$

We may also assign each $C\in V$ a non-negative cost $w(C)$ (e.g. the size of $C$), and given a total budget $W$, our goal is to find a solution of the following,
$$\argmax_{S\subseteq V: w(S) \leq W} f(S),$$
where $w(S) = \sum_{C\in S} w(C)$. This is a monotone submodular maximization problem under the knapsack constraint.



\paragraph{Maximum Spanning Forest}
Let us consider a graph $G = (V, E)$ where $V$ is the set of vertices and $E$ is the set of edges. In this case we consider $E$ as the ground set and define 
$$\calI = \{S\subseteq E ~|~\text{edge-induced graph}~G(V, S)~\text{that does not contain a circle}\}.$$

One can verify (via definition) that $(E, \calI)$ is a matriod. The rank function of $(E, \calI)$ can be interpreted as the size of the maximum spanning forest of an edge-induced graph, i.e. given $S\subseteq E$, $r(S)$ is the size of maximum spanning forest (in terms of number of edges) of $G(V, S)$. 




Now assume that we assign each $e\in E$ a weight $w_e \geq 0$. Let $f: 2^E\rightarrow \bbR$ with $f(S) = \sum_{e\in S}w_e$ be the objective function we want to maximize. Clearly $f$ is monotone and (sub)modular. we consider the following optimization problem,
$$\argmax_{S \in \calI} f(S).$$
This is exactly the \emph{Maximum Spanning Forest} problem and by Theorem \ref{thm:matroid}, we can solve it efficiently (and exactly) using Algorithm \ref{algo:greedy}.


\paragraph{Maximum Cut in Graphs}
Given an undirected graph $G = (V, E)$ and a non-negative capacity function $c: E \rightarrow \bbR^+\cup\{0\}$, the cut capacity function $f:2^V \rightarrow \bbR$ defined by $f(S) = \sum_{e\in \delta(S)} c(e)$ is submodular, where $\delta(S) = \{e\in E~|~e~\text{has exactly one vertex in}~S\}$ i.e. the set of edges crossing $S$ and $E\backslash S$. To shows why $f$ is submodular, we introduce an auxiliary function $f_e: 2^{\{u, v\}} \rightarrow \bbR$ for each $e = \{u, v\}\in E$ which is defined on the graph $G_e = (\{u,v\}, \{e\})$. We define, 
\begin{itemize}
\item $f_e(\{u, v\}) = f_e(\emptyset) = 0$
\item $f_e(\{u\}) = f_e(\{v\}) = w_e$
\end{itemize}
Then $f_e$ is submodular. We have,
$$f(S) = \sum_{e\in E} f_e(S \cap \{u, v\}).$$
The submodularity of $f$ follows Property \ref{prop:addition} and Property \ref{prop:restriction}.

An interesting optimization problem can then be formulated as 
$$\argmax_{S\subseteq E: |S| \leq k} f(S).$$





\subsection{Applications to NLP}
\paragraph{Summarization}
In the task of summarization, we are given a ground set $V$ and we want to find a subset of $V$ which maximizes some quality measurement under certain constraints. One popular formulation is that, given a function $f: 2^V \rightarrow \bbR$ that measures the quality of a summarization, we try to solve
$$\argmax_{S\in\calI} f(S),$$
where $\calI$ is a knapsack constraint.

Lin et al.\ \cite{LB11} pointed out that a lot of existing work for document summarization task fit the knapsack optimization framework. Furthermore the quality measurement functions being used are usually submodular. Lin et al. also proposed a class of submodular functions that outperform previous work in many aspects. Each of those functions combines two terms, one that encourages the summary to be representative of the corpus, and the other that positively rewards diversity. In the task of speech summarization, several submodular functions were discussed by Lin \cite{L12}. More discussion on the applications of submodular optimization to summarization can be found in \cite{L12}.

\paragraph{Word Alignment}
Word alignment is a key component in most statistical machine translation systems. Unlike classical approaches that utilize graphical models, Lin et al.\ \cite{LB11word} viewed word alignment problem as submodular maximization problem under matroid constraints. 

Suppose that we are given a source language (English) string $e_1, e_2, \ldots, e_n$ and a target language (French) string $f_1, f_2, \ldots, f_m$. Let $V = \{(i, j) ~|~ i\in[n], j\in[m] \}$ be the ground set. The goal is to find a matching $S\subseteq V$ that maximizes a certain quality function under some constraints. 

Let $P_1, \ldots, P_m$ be a partition of $V$ where $P_j = [n]\times \{j\}$. The \emph{fertility} restriction of a word $f_j$ then requires that $f_j$ can only match at most $k_j$ words in $e_1,\ldots, e_n$, or equivalently the matching $S \subseteq V$ satisfies $|S \cap P_j| \leq k_i$ for all $j\in[m]$. Such a constraint is called \emph{partition constraint}, which is an important instance of matroid. Lin et al. then proposed a submodular function as the quality measurement, which is a composition of a concave function and a modular function. We refer readers to \cite{LB11word} for details.





\subsection{Applications to Social Networks}
\paragraph{Influence Maximization}
Domingos and Richardson \cite{DR01,RD02} posed a fundamental algorithmic problem: if we can try to convince a subset of individuals (at most $k$) to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target? 
Kempe et al.\ \cite{KKT03} considered such question as a discrete optimization problem. Let $G = (V, E, W)$ be a social network (a directed graph with non-negative weights for each edge), two basic models of influence spreading were considered in \cite{KKT03}.
\begin{itemize}
\item Linear Threshold Mode: each $v\in V$ uniformly at random samples a threshold $\theta_v$ from some interval. After initializing $A \subset V$ as the set of active nodes, at each time step, any node $v$ with the total weights of its active neighbors above $\theta_v$ will be activated. Repeat this process until no more nodes can be activated. 

\item Independent Cascade Model:  after initializing the active set $A \subset V$, at each time step each active node $v$ will activate its neighbor $u$ (for all inactive neighbors) with probability $w_{v,u}$ (which is the parameter of this network).
\end{itemize}

Now let $\sigma(A)$ be the expected number of active nodes after long enough time. \cite{KKT03} proved that for both models, $\sigma: 2^V \rightarrow \bbZ$ is a monotone submodular function.


In this application, one is unlikely to able to efficiently compute the exact value of $\sigma(A)$. In fact, one can extend the result of Theorem \ref{thm:1978} to show that, by using  $(1 + O(\eps))$-approximate values for the function to be optimized, we obtain a $(1 - 1/e - \eps)$-approximation of the optimum.


\paragraph{Network Structure Inference}
Gomez Rodriguez et al.\ \cite{GLK10} first introduced submodular maximization to the context of network structure learning. They considered the problem of learning the network structure in an influence network: given a hidden directed network $G^*$ (vertices are known while edges are not), we observe multiple cascades spreading over it. Each cascade $c$ can be considered as a series of triples. Each triple has the form $(u_i, t_i, \phi_i)_c$ which presents the event that at time $t_i$, node $u_i$ is reached by $c$ with feature $\phi_i$. The goal is to infer the structure of $G^*$ based on a collection of cascades observed (denoted as $C$).

To model this process, they assumed that in each cascade, each node can only be influenced by at most one other node. So the influence structure of a cascade $c$ is given by a directed tree $T \subseteq G$. Let $P(c|T)$ be the probability of $c$ propagating in a particular tree pattern $T$, $P(c|G)$ the probability that cascade $c$ occurs in a network $G$. Let $\mathcal{T}(G)$ be set of all spanning trees of $G$. \cite{GLK10} then proposed the model 
$$P(c|G) = \max_{T\in\mathcal{T}(G)}P(c|T) = \max_{T\in\mathcal{T}(G)}\Pi_{(u,v)\in T}P_c(u,v)$$
where $P_c(u, v) \propto e^{-(t_v - t_u)}$. The structure of the network can be approximated by solving 
 $$\argmax_{|G|\leq k} P(C|G) = \argmax_{|G|\leq k}\Pi_{c\in C}P(c|G).$$


Consider the improvement of log-likelihood for cascade $c$ under graph $G$ over an empty graph $K$:
$$F_c(G) = \max_{T\in \mathcal{T}(G)}\log P(c|T) - \max_{T\in \mathcal{T}(K)}P(c|T).$$
It turns out that $F_c(G)$ can be expressed as 
$$F_c(G) = \max_{T\in\mathcal{T}(G)}\sum_{(u, v)\in T}w_c(u, v),$$
where $w_c(u, v)$ is a non-negative weight. Optimizing $P(C|G)$ is equivalently to optimize $F_C(G) = \sum_{c\in C}F_c(G)$ and the latter is shown to be submodular (the ground set is all possible graphs defined over the given vertices set). 






\subsection{Applications to Machine Learning}
\paragraph{Kernel Machines}
Kernel machines \cite{SS02} are powerful non-parametric learning techniques. Those approaches use kernel trick to reduce non-linear problems to linear tasks that have been well studied. The data set $V = \{x_1, x_2, \ldots, x_n\}$ is represented in a transformed space via a kernel matrix

\[ K_V = \left( \begin{array}{ccc}
\calK(x_1, x_2) & \ldots & \calK(x_1, x_n) \\
\vdots & \ddots & \vdots \\
\calK(x_n,x_1) & \ldots & \calK(x_n, x_n) \end{array} \right),\] 
where $\calK: V\times V \rightarrow \bbR$ is the kernel function that is symmetric and positive definite. 

For large-scale problem, even representing the matrix $K_V$ (which requires $O(n^2)$ space) is prohibited. The common solution to solve this issue is to select a small representative subset $S \subseteq V$ and only work with $K_S$. An additional benefit by using such a subset is that it may help to avoid overfitting. The question now becomes how to select the subset $S$. One popular way to measure the quality of selected set $S$ is to use \emph{Informative Vector Machine}(IVM) introduced by Laurence et al.\ \cite{LSH03}. Formally, we define $f: 2^V \rightarrow \bbR$ with
$$f(S) = \frac{1}{2} \log\det\left( \mathbf{I} + \sigma^{-2} K_S \right),$$
where $\mathbf{I}$ is the identity matrix and $\sigma > 0$ is a parameter. IVM has a close connection to the entropy of  muti-variable Gaussian distribution \cite{B14} and it is shown in \cite{S04,B14} that $f$ is a submodular function. We can then select the set $S\subset V$ by solving
$$\argmax_{S:|S|\leq k} f(S).$$

%\chensays{consider to add another example for ML}

% \subsection{More Applications}



% \begin{itemize}
% \item \cite{MKC+15} distributed k-centers using submodular optimization.
% \end{itemize}

